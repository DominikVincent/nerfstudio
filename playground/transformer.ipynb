{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(torch.nn.Module):\n",
    "    def __init__(self, output_size, num_layers, d_model, num_heads, dff, dropout_rate):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define the transformer encoder\n",
    "        self.encoder_layer = torch.nn.TransformerEncoderLayer(d_model, num_heads, dff, dropout_rate, batch_first=True)\n",
    "        self.transformer_encoder = torch.nn.TransformerEncoder(self.encoder_layer, num_layers)\n",
    "\n",
    "        # Define the output layer\n",
    "        self.final_layer = torch.nn.Linear(d_model, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the transformer encoder\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        # Apply the final layer\n",
    "        x = self.final_layer(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250182\n",
      "torch.Size([16, 32000, 128])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:66] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 262144000000 bytes. Error code 12 (Cannot allocate memory)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39m# Pass the input data through the model to get the output\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39mprint\u001b[39m(input_data\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> 24\u001b[0m output_data \u001b[39m=\u001b[39m model(input_data)\n\u001b[1;32m     25\u001b[0m \u001b[39m# expect 16, 10, 6\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[39mprint\u001b[39m(output_data\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m/data/vision/polina/projects/wmh/dhollidt/conda/envs/nerfstudio3/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[25], line 14\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     13\u001b[0m     \u001b[39m# Apply the transformer encoder\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer_encoder(x)\n\u001b[1;32m     16\u001b[0m     \u001b[39m# Apply the final layer\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinal_layer(x)\n",
      "File \u001b[0;32m/data/vision/polina/projects/wmh/dhollidt/conda/envs/nerfstudio3/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/vision/polina/projects/wmh/dhollidt/conda/envs/nerfstudio3/lib/python3.8/site-packages/torch/nn/modules/transformer.py:238\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    236\u001b[0m         output \u001b[39m=\u001b[39m mod(output, src_mask\u001b[39m=\u001b[39mmask)\n\u001b[1;32m    237\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 238\u001b[0m         output \u001b[39m=\u001b[39m mod(output, src_mask\u001b[39m=\u001b[39;49mmask, src_key_padding_mask\u001b[39m=\u001b[39;49msrc_key_padding_mask)\n\u001b[1;32m    240\u001b[0m \u001b[39mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m    241\u001b[0m     output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mto_padded_tensor(\u001b[39m0.\u001b[39m)\n",
      "File \u001b[0;32m/data/vision/polina/projects/wmh/dhollidt/conda/envs/nerfstudio3/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/vision/polina/projects/wmh/dhollidt/conda/envs/nerfstudio3/lib/python3.8/site-packages/torch/nn/modules/transformer.py:463\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    461\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x))\n\u001b[1;32m    462\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 463\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sa_block(x, src_mask, src_key_padding_mask))\n\u001b[1;32m    464\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(x))\n\u001b[1;32m    466\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/data/vision/polina/projects/wmh/dhollidt/conda/envs/nerfstudio3/lib/python3.8/site-packages/torch/nn/modules/transformer.py:471\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sa_block\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor,\n\u001b[1;32m    470\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 471\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(x, x, x,\n\u001b[1;32m    472\u001b[0m                        attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[1;32m    473\u001b[0m                        key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[1;32m    474\u001b[0m                        need_weights\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    475\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout1(x)\n",
      "File \u001b[0;32m/data/vision/polina/projects/wmh/dhollidt/conda/envs/nerfstudio3/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/vision/polina/projects/wmh/dhollidt/conda/envs/nerfstudio3/lib/python3.8/site-packages/torch/nn/modules/activation.py:1153\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights)\u001b[0m\n\u001b[1;32m   1142\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1143\u001b[0m         query, key, value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\n\u001b[1;32m   1144\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         q_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_proj_weight, k_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_proj_weight,\n\u001b[1;32m   1151\u001b[0m         v_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_proj_weight, average_attn_weights\u001b[39m=\u001b[39maverage_attn_weights)\n\u001b[1;32m   1152\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1153\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mmulti_head_attention_forward(\n\u001b[1;32m   1154\u001b[0m         query, key, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[1;32m   1155\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[1;32m   1156\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_v, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_zero_attn,\n\u001b[1;32m   1157\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m   1158\u001b[0m         training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[1;32m   1159\u001b[0m         key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask, need_weights\u001b[39m=\u001b[39;49mneed_weights,\n\u001b[1;32m   1160\u001b[0m         attn_mask\u001b[39m=\u001b[39;49mattn_mask, average_attn_weights\u001b[39m=\u001b[39;49maverage_attn_weights)\n\u001b[1;32m   1161\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m is_batched:\n\u001b[1;32m   1162\u001b[0m     \u001b[39mreturn\u001b[39;00m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m/data/vision/polina/projects/wmh/dhollidt/conda/envs/nerfstudio3/lib/python3.8/site-packages/torch/nn/functional.py:5179\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights)\u001b[0m\n\u001b[1;32m   5174\u001b[0m     dropout_p \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m   5176\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m   5177\u001b[0m \u001b[39m# (deep breath) calculate attention and out projection\u001b[39;00m\n\u001b[1;32m   5178\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m-> 5179\u001b[0m attn_output, attn_output_weights \u001b[39m=\u001b[39m _scaled_dot_product_attention(q, k, v, attn_mask, dropout_p)\n\u001b[1;32m   5180\u001b[0m attn_output \u001b[39m=\u001b[39m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(tgt_len \u001b[39m*\u001b[39m bsz, embed_dim)\n\u001b[1;32m   5181\u001b[0m attn_output \u001b[39m=\u001b[39m linear(attn_output, out_proj_weight, out_proj_bias)\n",
      "File \u001b[0;32m/data/vision/polina/projects/wmh/dhollidt/conda/envs/nerfstudio3/lib/python3.8/site-packages/torch/nn/functional.py:4854\u001b[0m, in \u001b[0;36m_scaled_dot_product_attention\u001b[0;34m(q, k, v, attn_mask, dropout_p)\u001b[0m\n\u001b[1;32m   4852\u001b[0m     attn \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mbaddbmm(attn_mask, q, k\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m   4853\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 4854\u001b[0m     attn \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mbmm(q, k\u001b[39m.\u001b[39;49mtranspose(\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[1;32m   4856\u001b[0m attn \u001b[39m=\u001b[39m softmax(attn, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m   4857\u001b[0m \u001b[39mif\u001b[39;00m dropout_p \u001b[39m>\u001b[39m \u001b[39m0.0\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:66] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 262144000000 bytes. Error code 12 (Cannot allocate memory)"
     ]
    }
   ],
   "source": [
    "# Define the input and output sizes\n",
    "input_size = 32000\n",
    "output_size = 6\n",
    "\n",
    "# Define the transformer model\n",
    "num_layers = 2\n",
    "d_model = 128\n",
    "num_heads = 4\n",
    "dff = 64\n",
    "dropout_rate = 0.1\n",
    "model = TransformerModel(output_size, num_layers, d_model, num_heads, dff, dropout_rate)\n",
    "\n",
    "# Generate some example input data\n",
    "batch_size = 16\n",
    "seq_length = input_size\n",
    "input_data = torch.randn(batch_size, seq_length, d_model)\n",
    "\n",
    "# print the parameters of the model\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(total_params)\n",
    "\n",
    "# Pass the input data through the model to get the output\n",
    "print(input_data.shape)\n",
    "output_data = model(input_data)\n",
    "# expect 16, 10, 6\n",
    "print(output_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "src = torch.rand(10, 32, 512)\n",
    "out = transformer_encoder(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 32, 512])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 32, 512])\n",
      "tensor([[[0.8831, 0.7192, 0.7406,  ..., 0.8938, 0.6347, 0.5211],\n",
      "         [0.2122, 0.4283, 0.1966,  ..., 0.2230, 0.0964, 0.0918],\n",
      "         [0.7867, 0.8161, 0.5503,  ..., 0.8816, 0.2052, 0.5661],\n",
      "         ...,\n",
      "         [0.4690, 0.1957, 0.8169,  ..., 0.3704, 0.0157, 0.3309],\n",
      "         [0.3612, 0.8310, 0.9841,  ..., 0.4858, 0.4641, 0.8028],\n",
      "         [0.0825, 0.4360, 0.0565,  ..., 0.1524, 0.3171, 0.8398]],\n",
      "\n",
      "        [[0.4719, 0.3796, 0.5296,  ..., 0.8090, 0.1761, 0.2643],\n",
      "         [0.0157, 0.2695, 0.6866,  ..., 0.6252, 0.8721, 0.7033],\n",
      "         [0.2062, 0.2626, 0.8744,  ..., 0.4858, 0.9292, 0.9802],\n",
      "         ...,\n",
      "         [0.1701, 0.1103, 0.1372,  ..., 0.7573, 0.0362, 0.4542],\n",
      "         [0.7662, 0.3510, 0.6453,  ..., 0.9619, 0.7399, 0.8724],\n",
      "         [0.3191, 0.2178, 0.6184,  ..., 0.6606, 0.6076, 0.3059]],\n",
      "\n",
      "        [[0.7307, 0.7800, 0.7666,  ..., 0.7604, 0.5445, 0.5923],\n",
      "         [0.7473, 0.8057, 0.5010,  ..., 0.7684, 0.9257, 0.0859],\n",
      "         [0.9730, 0.5786, 0.2559,  ..., 0.9745, 0.0030, 0.4733],\n",
      "         ...,\n",
      "         [0.4746, 0.5789, 0.8716,  ..., 0.2628, 0.1958, 0.4416],\n",
      "         [0.7325, 0.6984, 0.7170,  ..., 0.5765, 0.9003, 0.7987],\n",
      "         [0.2070, 0.3274, 0.0626,  ..., 0.6010, 0.3080, 0.2562]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.8402, 0.6093, 0.5041,  ..., 0.4715, 0.9849, 0.1654],\n",
      "         [0.2414, 0.0331, 0.2373,  ..., 0.0849, 0.8827, 0.7494],\n",
      "         [0.8596, 0.8501, 0.4687,  ..., 0.2218, 0.7886, 0.1047],\n",
      "         ...,\n",
      "         [0.8071, 0.7259, 0.5345,  ..., 0.7954, 0.1646, 0.5804],\n",
      "         [0.5150, 0.4425, 0.2821,  ..., 0.2428, 0.4609, 0.8516],\n",
      "         [0.8153, 0.5537, 0.7411,  ..., 0.7397, 0.9300, 0.4925]],\n",
      "\n",
      "        [[0.6043, 0.2235, 0.4951,  ..., 0.8776, 0.9816, 0.7597],\n",
      "         [0.6402, 0.9666, 0.0659,  ..., 0.3715, 0.8840, 0.8624],\n",
      "         [0.5050, 0.6450, 0.1254,  ..., 0.4845, 0.8447, 0.2562],\n",
      "         ...,\n",
      "         [0.7338, 0.2147, 0.2836,  ..., 0.7848, 0.1017, 0.4751],\n",
      "         [0.4700, 0.9877, 0.1918,  ..., 0.1446, 0.4324, 0.2653],\n",
      "         [0.5109, 0.2694, 0.0376,  ..., 0.8411, 0.0447, 0.2143]],\n",
      "\n",
      "        [[0.8009, 0.2093, 0.8261,  ..., 0.5041, 0.7182, 0.7480],\n",
      "         [0.3643, 0.1058, 0.3999,  ..., 0.0689, 0.5644, 0.4814],\n",
      "         [0.0850, 0.7235, 0.1892,  ..., 0.2571, 0.3210, 0.2737],\n",
      "         ...,\n",
      "         [0.1516, 0.2648, 0.2774,  ..., 0.9806, 0.7085, 0.1631],\n",
      "         [0.1934, 0.4440, 0.4751,  ..., 0.9542, 0.8035, 0.2331],\n",
      "         [0.7790, 0.1419, 0.1260,  ..., 0.4062, 0.3704, 0.1368]]])\n",
      "tensor([[[ 2.5746e+00, -8.0230e-01,  2.0076e+00,  ..., -2.2814e-01,\n",
      "           2.8878e-01, -6.0285e-01],\n",
      "         [ 1.4478e+00, -7.8584e-02,  1.6759e+00,  ..., -2.4033e-02,\n",
      "           2.3275e-01, -2.2146e+00],\n",
      "         [ 2.4374e+00, -8.8890e-01,  1.1874e+00,  ..., -6.7550e-01,\n",
      "          -5.0762e-01, -9.6921e-01],\n",
      "         ...,\n",
      "         [ 1.6500e+00, -4.8093e-01,  2.2919e+00,  ..., -3.3334e-01,\n",
      "           7.1427e-01, -1.1015e+00],\n",
      "         [ 1.7793e+00,  1.0455e-01,  2.6299e+00,  ...,  4.1233e-01,\n",
      "          -1.0750e-01, -1.5533e+00],\n",
      "         [ 2.8255e+00, -1.3363e+00,  1.8914e+00,  ..., -7.4848e-03,\n",
      "           7.8254e-01, -1.1788e+00]],\n",
      "\n",
      "        [[ 2.6849e+00, -1.1922e+00,  1.6256e+00,  ..., -5.4887e-01,\n",
      "          -2.8006e-01, -1.7983e+00],\n",
      "         [ 1.5183e+00,  2.6389e-01,  2.0236e+00,  ...,  7.9062e-01,\n",
      "           1.7009e-02, -1.9149e+00],\n",
      "         [ 2.0357e+00, -1.7843e+00,  1.3524e+00,  ..., -8.8080e-01,\n",
      "          -5.7793e-01, -5.4684e-01],\n",
      "         ...,\n",
      "         [ 2.3586e+00, -5.6607e-01,  1.4165e+00,  ..., -1.2408e+00,\n",
      "           2.1409e-01, -1.1583e+00],\n",
      "         [ 2.0614e+00, -5.1045e-01,  1.0964e+00,  ..., -4.4011e-01,\n",
      "          -3.5406e-01, -1.3765e+00],\n",
      "         [ 1.9775e+00, -5.5462e-01,  1.7686e+00,  ..., -2.1881e-01,\n",
      "           4.6642e-01, -1.1361e+00]],\n",
      "\n",
      "        [[ 2.7407e+00, -8.7334e-01,  1.3197e+00,  ..., -4.2676e-01,\n",
      "          -4.0815e-01, -2.0905e+00],\n",
      "         [ 1.2528e+00, -2.2261e-01,  1.5048e+00,  ...,  3.3825e-01,\n",
      "           2.3263e-01, -1.8475e+00],\n",
      "         [ 2.3730e+00, -1.5005e+00,  2.1592e+00,  ...,  3.8029e-01,\n",
      "          -2.7655e-01, -7.2319e-01],\n",
      "         ...,\n",
      "         [ 1.3735e+00, -7.3751e-01,  1.4160e+00,  ..., -1.9409e+00,\n",
      "           7.8733e-03, -1.8905e+00],\n",
      "         [ 1.5037e+00,  1.5400e-01,  1.4269e+00,  ...,  9.7967e-02,\n",
      "           5.2790e-01, -1.1756e+00],\n",
      "         [ 2.2812e+00, -2.5113e-01,  1.7431e+00,  ..., -4.4820e-01,\n",
      "          -6.0089e-01, -1.3859e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.4566e+00, -9.6611e-01,  2.5160e+00,  ..., -1.8848e-01,\n",
      "           4.5240e-01, -5.8747e-01],\n",
      "         [ 6.9127e-01, -8.0562e-02,  1.6816e+00,  ...,  5.3182e-01,\n",
      "           1.5737e-01, -1.6311e+00],\n",
      "         [ 2.9123e+00, -7.9887e-01,  1.5643e+00,  ...,  2.9464e-01,\n",
      "          -5.5512e-01, -7.7312e-01],\n",
      "         ...,\n",
      "         [ 2.5482e+00, -4.5581e-01,  1.5588e+00,  ..., -6.5107e-01,\n",
      "           6.9382e-01, -6.1376e-01],\n",
      "         [ 2.6270e+00, -5.1537e-01,  2.1710e+00,  ..., -3.0960e-01,\n",
      "          -5.6613e-01, -1.3642e+00],\n",
      "         [ 1.4768e+00, -1.1856e+00,  2.1027e+00,  ..., -2.9713e-03,\n",
      "           1.1058e-02, -2.2233e-01]],\n",
      "\n",
      "        [[ 2.8159e+00, -1.0875e+00,  1.8576e+00,  ..., -7.0021e-01,\n",
      "           6.1266e-01, -2.0924e-01],\n",
      "         [ 4.6857e-01, -5.3402e-01,  1.8084e+00,  ..., -1.0627e-01,\n",
      "           3.0961e-01, -1.1643e+00],\n",
      "         [ 2.2224e+00, -1.3637e+00,  1.7314e+00,  ...,  1.4527e-01,\n",
      "          -8.2232e-01, -9.3835e-01],\n",
      "         ...,\n",
      "         [ 2.1674e+00, -9.7223e-01,  8.9994e-01,  ..., -1.4217e+00,\n",
      "           1.6278e-01, -1.4502e+00],\n",
      "         [ 1.6589e+00,  2.7809e-01,  1.4853e+00,  ..., -6.5734e-01,\n",
      "           4.0079e-01, -9.6012e-02],\n",
      "         [ 3.1187e+00, -7.3649e-01,  2.4654e+00,  ..., -8.3369e-01,\n",
      "          -5.8940e-01, -9.2816e-01]],\n",
      "\n",
      "        [[ 2.2705e+00, -4.3950e-01,  2.7269e+00,  ..., -3.3335e-01,\n",
      "           9.1416e-02, -9.4678e-01],\n",
      "         [ 1.1487e+00, -2.1898e-01,  1.4759e+00,  ..., -3.1562e-01,\n",
      "           6.0291e-01, -6.8027e-01],\n",
      "         [ 1.9353e+00, -1.0521e+00,  1.4139e+00,  ..., -1.6311e-01,\n",
      "          -1.3940e+00, -2.0977e+00],\n",
      "         ...,\n",
      "         [ 7.0817e-01,  2.1777e-01,  1.1347e+00,  ..., -2.0312e-01,\n",
      "          -1.1823e-01, -1.8717e+00],\n",
      "         [ 7.3809e-01, -1.0444e-01,  1.5679e+00,  ...,  1.3519e-02,\n",
      "           6.7285e-01, -1.5204e+00],\n",
      "         [ 2.1103e+00,  1.3498e-01,  1.9555e+00,  ..., -5.3918e-01,\n",
      "           2.8270e-01, -6.9256e-01]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)\n",
    "src = torch.rand((10, 32, 512))\n",
    "tgt = torch.rand((20, 32, 512))\n",
    "out = transformer_model(src, tgt)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nerfstudio3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d982505c67f6491cc57124614a47f97cb6b2fba9cbe418d2edc6b5ed45f83d2f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
