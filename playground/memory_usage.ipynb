{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "from nerfstudio.models.nesf import TransformerEncoderModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TransformerEncoderModel(\n",
    "                input_size=48,\n",
    "                feature_dim=64,\n",
    "                num_layers=4,\n",
    "                num_heads=8,\n",
    "                dim_feed_forward=64,\n",
    "                dropout_rate=0.1,\n",
    "                activation=torch.nn.ReLU(),\n",
    "                pretrain=False,\n",
    "                mask_ratio=0.5,\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "TransformerEncoderModel                       [1, 3000, 64]             25,216\n",
       "├─Sequential: 1-1                             [1, 3000, 64]             --\n",
       "│    └─Linear: 2-1                            [1, 3000, 64]             3,136\n",
       "│    └─ReLU: 2-2                              [1, 3000, 64]             --\n",
       "├─TransformerEncoder: 1-2                     [1, 3000, 64]             --\n",
       "│    └─ModuleList: 2-3                        --                        --\n",
       "│    │    └─TransformerEncoderLayer: 3-1      [1, 3000, 64]             25,216\n",
       "│    │    └─TransformerEncoderLayer: 3-2      [1, 3000, 64]             25,216\n",
       "│    │    └─TransformerEncoderLayer: 3-3      [1, 3000, 64]             25,216\n",
       "│    │    └─TransformerEncoderLayer: 3-4      [1, 3000, 64]             25,216\n",
       "├─ReLU: 1-3                                   [1, 3000, 64]             --\n",
       "===============================================================================================\n",
       "Total params: 129,216\n",
       "Trainable params: 129,216\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.00\n",
       "===============================================================================================\n",
       "Input size (MB): 0.58\n",
       "Forward/backward pass size (MB): 1.54\n",
       "Params size (MB): 0.01\n",
       "Estimated Total Size (MB): 2.12\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(transformer, input_size=(1, 3000, 48))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   45571 KB |  331133 KB |    1181 MB |    1136 MB |\n",
      "|       from large pool |   45056 KB |  328556 KB |    1160 MB |    1116 MB |\n",
      "|       from small pool |     515 KB |    4851 KB |      21 MB |      20 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   45571 KB |  331133 KB |    1181 MB |    1136 MB |\n",
      "|       from large pool |   45056 KB |  328556 KB |    1160 MB |    1116 MB |\n",
      "|       from small pool |     515 KB |    4851 KB |      21 MB |      20 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  354304 KB |  354304 KB |  354304 KB |       0 B  |\n",
      "|       from large pool |  348160 KB |  348160 KB |  348160 KB |       0 B  |\n",
      "|       from small pool |    6144 KB |    6144 KB |    6144 KB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |    1533 KB |   23170 KB |  111550 KB |  110017 KB |\n",
      "|       from large pool |       0 KB |   19604 KB |   87416 KB |   87416 KB |\n",
      "|       from small pool |    1533 KB |    3566 KB |   24134 KB |   22601 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |      63    |      71    |     119    |      56    |\n",
      "|       from large pool |       1    |       3    |      13    |      12    |\n",
      "|       from small pool |      62    |      70    |     106    |      44    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |      63    |      71    |     119    |      56    |\n",
      "|       from large pool |       1    |       3    |      13    |      12    |\n",
      "|       from small pool |      62    |      70    |     106    |      44    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       6    |       6    |       6    |       0    |\n",
      "|       from large pool |       3    |       3    |       3    |       0    |\n",
      "|       from small pool |       3    |       3    |       3    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       2    |       7    |      31    |      29    |\n",
      "|       from large pool |       0    |       3    |      12    |      12    |\n",
      "|       from small pool |       2    |       4    |      19    |      17    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nerfstudio3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d982505c67f6491cc57124614a47f97cb6b2fba9cbe418d2edc6b5ed45f83d2f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
